{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import json\n",
    "import sys\n",
    "import praw\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1577817000\n",
      "1583001000\n",
      "Saved 100 submissions through 2020-02-23\n",
      "Saved 200 submissions through 2020-02-16\n",
      "Saved 300 submissions through 2020-02-09\n",
      "Saved 400 submissions through 2020-02-01\n",
      "Saved 500 submissions through 2020-01-27\n",
      "Saved 600 submissions through 2020-01-21\n",
      "Saved 700 submissions through 2020-01-14\n",
      "Saved 800 submissions through 2020-01-07\n",
      "Saved 900 submissions through 2020-01-01\n",
      "Saved 903 submissions through 2020-01-01\n",
      "Saved 903 submissions\n"
     ]
    }
   ],
   "source": [
    "# Scraping data for subreddit \"r/emacs\" [01/01/2020 - 03/01/2020]\n",
    "\n",
    "def downloadSubreddit(filename, subreddit, start_time, end_time):\n",
    "    submissions = []\n",
    "    count = 0\n",
    "    submissions_with_metrics = []\n",
    "    previous_epoch = int(start_time.timestamp())\n",
    "    after_epoch = int(end_time.timestamp())\n",
    "    print(after_epoch)\n",
    "    print(previous_epoch)\n",
    "    \n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?subreddit={}&after={}&before={}&sort=desc&limit=1000'\n",
    "    reddit = praw.Reddit(client_id='_87SCviJLvIICA', client_secret='yio0cnfuq97lMDQRPfvXSfje-O-z_Q', user_agent='redditdataanalysis')\n",
    "    \n",
    "    while True:\n",
    "        new_url = url.format(subreddit, after_epoch, previous_epoch)\n",
    "        json_text = requests.get(new_url, headers={'User-Agent': \"Post downloader by /u/nsajnani\"})\n",
    "        time.sleep(1)  # pushshift has a rate limit, if we send requests too fast it will start returning error messages\n",
    "        try:\n",
    "            json_data = json_text.json()\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        if 'data' not in json_data:\n",
    "            break\n",
    "        objects = json_data['data']\n",
    "        if len(objects) == 0:\n",
    "            break\n",
    "\n",
    "        for object in objects:\n",
    "            previous_epoch = object['created_utc'] - 1\n",
    "            count += 1\n",
    "            praw_submission = reddit.submission(id=object['id'])\n",
    "            submissions_with_metrics.append([praw_submission.author, praw_submission.title, praw_submission.upvote_ratio, praw_submission.is_original_content, praw_submission.edited, praw_submission.score, praw_submission.id, praw_submission.subreddit, praw_submission.url, praw_submission.num_comments, praw_submission.selftext, praw_submission.created])\n",
    "\n",
    "\n",
    "        print(\"Saved {} submissions through {}\".format(count, datetime.fromtimestamp(previous_epoch).strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    print(f\"Saved {count} submissions\")\n",
    "    submissions_with_metrics = pd.DataFrame(submissions_with_metrics,columns=['author','title', 'upvote_ratio','is_original_content','edited','score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "    submissions_with_metrics.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "downloadSubreddit(\"submissions_with_metrics_emacs.csv\", \"emacs\", datetime(2020, 3, 1), datetime(2020, 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1577817000\n",
      "1583001000\n",
      "Saved 100 submissions through 2020-02-22\n",
      "Saved 200 submissions through 2020-02-14\n",
      "Saved 300 submissions through 2020-02-06\n",
      "Saved 400 submissions through 2020-01-28\n",
      "Saved 500 submissions through 2020-01-21\n",
      "Saved 600 submissions through 2020-01-13\n",
      "Saved 700 submissions through 2020-01-04\n",
      "Saved 733 submissions through 2020-01-01\n",
      "Saved 733 submissions\n"
     ]
    }
   ],
   "source": [
    "# Scraping data for subreddit \"r/vim\" [01/01/2020 - 03/01/2020]\n",
    "\n",
    "def downloadSubreddit(filename, subreddit, start_time, end_time):\n",
    "    submissions = []\n",
    "    count = 0\n",
    "    submissions_with_metrics = []\n",
    "    previous_epoch = int(start_time.timestamp())\n",
    "    after_epoch = int(end_time.timestamp())\n",
    "    print(after_epoch)\n",
    "    print(previous_epoch)\n",
    "    \n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?subreddit={}&after={}&before={}&sort=desc&limit=1000'\n",
    "    reddit = praw.Reddit(client_id='_87SCviJLvIICA', client_secret='yio0cnfuq97lMDQRPfvXSfje-O-z_Q', user_agent='redditdataanalysis')\n",
    "    \n",
    "    while True:\n",
    "        new_url = url.format(subreddit, after_epoch, previous_epoch)\n",
    "        json_text = requests.get(new_url, headers={'User-Agent': \"Post downloader by /u/nsajnani\"})\n",
    "        time.sleep(1)  # pushshift has a rate limit, if we send requests too fast it will start returning error messages\n",
    "        try:\n",
    "            json_data = json_text.json()\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "        if 'data' not in json_data:\n",
    "            break\n",
    "        objects = json_data['data']\n",
    "        if len(objects) == 0:\n",
    "            break\n",
    "\n",
    "        for object in objects:\n",
    "            previous_epoch = object['created_utc'] - 1\n",
    "            count += 1\n",
    "            praw_submission = reddit.submission(id=object['id'])\n",
    "            submissions_with_metrics.append([praw_submission.author, praw_submission.title, praw_submission.upvote_ratio, praw_submission.is_original_content, praw_submission.edited, praw_submission.score, praw_submission.id, praw_submission.subreddit, praw_submission.url, praw_submission.num_comments, praw_submission.selftext, praw_submission.created])\n",
    "\n",
    "\n",
    "        print(\"Saved {} submissions through {}\".format(count, datetime.fromtimestamp(previous_epoch).strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "    print(f\"Saved {count} submissions\")\n",
    "    submissions_with_metrics = pd.DataFrame(submissions_with_metrics,columns=['author','title', 'upvote_ratio','is_original_content','edited','score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "    submissions_with_metrics.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "downloadSubreddit(\"submissions_with_metrics_vim.csv\", \"vim\", datetime(2020, 3, 1), datetime(2020, 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
